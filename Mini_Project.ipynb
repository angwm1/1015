{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae216661-9c0b-41af-9f76-be709c125663",
   "metadata": {},
   "source": [
    "#### Dataset source: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448d19a-b622-4d34-ac3c-037c6d18d8d6",
   "metadata": {},
   "source": [
    "#### Problem Statement: Given the heart disease dataset, our objective is to develop a machine learning model that can predict the presence of heart disease in patients based on various medical and demographic features. This could help in early diagnosis and inform medical professionals to take preventative or remedial action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760dd0c-88ee-47ed-8872-637d450e28b8",
   "metadata": {},
   "source": [
    "### data preparation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c636bd5-97b5-4c70-804d-6860978723f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T10:26:29.728622200Z",
     "start_time": "2024-04-13T10:26:29.322669900Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heart_disease_uci.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stats\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 加载数据集\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m heart_disease_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mheart_disease_uci.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdrop([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 1. 缺失值处理\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 数值型特征\u001B[39;00m\n\u001B[0;32m     13\u001B[0m numeric_features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrestbps\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchol\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthalch\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moldpeak\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mca\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'heart_disease_uci.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy import stats\n",
    "\n",
    "# 加载数据集\n",
    "heart_disease_data = pd.read_csv('heart_disease_uci.csv').drop(['id', 'dataset'], axis=1)\n",
    "\n",
    "\n",
    "# 1. 缺失值处理\n",
    "# 数值型特征\n",
    "numeric_features = ['trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "heart_disease_data[numeric_features] = numeric_imputer.fit_transform(heart_disease_data[numeric_features])\n",
    "\n",
    "# 分类特征\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "heart_disease_data[categorical_features] = categorical_imputer.fit_transform(heart_disease_data[categorical_features])\n",
    "\n",
    "# 2. 异常值检测与处理\n",
    "for feature in numeric_features:\n",
    "    Q1 = heart_disease_data[feature].quantile(0.25)\n",
    "    Q3 = heart_disease_data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # 将异常值替换为边界值\n",
    "    heart_disease_data[feature] = np.where(heart_disease_data[feature] < lower_bound, lower_bound, heart_disease_data[feature])\n",
    "    heart_disease_data[feature] = np.where(heart_disease_data[feature] > upper_bound, upper_bound, heart_disease_data[feature])\n",
    "\n",
    "# 3. 特征编码\n",
    "# One-Hot Encoding非序数分类特征\n",
    "heart_disease_data_encoded = pd.get_dummies(heart_disease_data, columns=categorical_features)\n",
    "\n",
    "# 4. 数据标准化/归一化\n",
    "# 更新后的数值型特征列表，确保仅包含原始的数值型特征\n",
    "numeric_features_updated = ['trestbps', 'chol', 'thalch', 'oldpeak', 'ca', 'age']\n",
    "scaler = StandardScaler()\n",
    "heart_disease_data_encoded[numeric_features_updated] = scaler.fit_transform(heart_disease_data[numeric_features_updated])\n",
    "\n",
    "# 检查处理后的数据\n",
    "heart_disease_data_encoded.info()\n",
    "heart_disease_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploratory data analysis/visualization to gather relevant insights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a57b520e650469"
  },
  {
   "cell_type": "markdown",
   "id": "a1939250-41ce-40d2-8804-c220ee8367a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b782e1-83c6-42bf-aa85-735c9f018833",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.387361300Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 并且确认'data/heart_disease_uci.csv'路径正确，且目标变量列名为'num'\n",
    "\n",
    "\n",
    "# 设置绘图风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Part 1: 单变量分析 - 数值型特征的高级分布统计和可视化\n",
    "numeric_features = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.histplot(heart_disease_data[feature], kde=True, color='skyblue')\n",
    "    plt.title(f'Distribution of {feature}', fontsize=14)\n",
    "    plt.axvline(heart_disease_data[feature].mean(), color='red', linestyle='--', label='Mean')\n",
    "    plt.axvline(heart_disease_data[feature].median(), color='green', linestyle='-', label='Median')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 分类特征的分布可视化\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(18, 24))\n",
    "axes = axes.flatten()  # 展平axes数组以便于迭代\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.countplot(x=feature, data=heart_disease_data, ax=axes[i], palette=\"Set2\")\n",
    "    axes[i].set_title(f'Distribution of {feature}', fontsize=14)\n",
    "for ax in axes[i+1:]:  # 移除多余的子图位置\n",
    "    ax.remove()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Part 2: 多变量分析 - 散点图矩阵：数值型特征间的关系以及它们与目标变量的关系\n",
    "sns.pairplot(heart_disease_data[numeric_features + ['num']], hue='num')\n",
    "plt.show()\n",
    "\n",
    "# 分类特征与心脏病的关系：百分比堆叠柱状图\n",
    "for feature in categorical_features:\n",
    "    pd.crosstab(heart_disease_data[feature], heart_disease_data['num']).apply(lambda r: r/r.sum(), axis=1).plot(kind='bar', stacked=True, colormap='Set2')\n",
    "    plt.title(f'{feature} vs Heart Disease')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 相关性热图：数值型特征与心脏病之间的相关性\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = heart_disease_data[numeric_features + ['num']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', fmt=\".2f\", center=0)\n",
    "plt.title('Correlation Matrix with Heart Disease', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc465f-704c-4b6c-a225-8ccc7b2131b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.388412100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假设'heart_disease_data'已经包含了你的数据\n",
    "# 对分类特征进行One-Hot Encoding\n",
    "heart_disease_data_encoded = pd.get_dummies(heart_disease_data, drop_first=True)\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = heart_disease_data_encoded.drop('num', axis=1)\n",
    "y = heart_disease_data_encoded['num']\n",
    "\n",
    "# 数据划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 实例化随机森林分类器\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性并与特征名配对\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=feature_importances['importance'], y=feature_importances.index)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "the use of machine learning techniques to solve specific problem"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5979b4154147602a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假设已经加载了数据到 heart_disease_data DataFrame\n",
    "# data_path = '/path/to/your/data.csv'\n",
    "# heart_disease_data = pd.read_csv(data_path)\n",
    "\n",
    "# 转换目标变量为二元变量\n",
    "heart_disease_data['heart_disease'] = (heart_disease_data['num'] > 0).astype(int)\n",
    "\n",
    "# 定义特征和目标变量\n",
    "X = heart_disease_data.drop(['heart_disease', 'num'], axis=1)  # 假设'drop'中的列名正确\n",
    "y = heart_disease_data['heart_disease']\n",
    "\n",
    "# 分离数值型和分类型特征\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# 创建预处理转换器\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 创建预处理步骤\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 应用预处理和PCA\n",
    "pipeline = make_pipeline(preprocessor, PCA(n_components=2))\n",
    "X_pca = pipeline.fit_transform(X)\n",
    "\n",
    "# 可视化前两个主成分\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
    "plt.title('PCA of Heart Disease Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Heart Disease', loc='best')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.389616700Z"
    }
   },
   "id": "692dc8dc81ae5d1d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13ff01c3-760e-489f-a8a6-7f11738645c2"
  },
  {
   "cell_type": "markdown",
   "id": "facb6354-b10e-45ec-bec0-b74cbbb1f4fe",
   "metadata": {},
   "source": [
    "#### trestbps (resting blood pressure): The distribution is slightly right-skewed, suggesting that a subset of the population has high blood pressure, which is a known risk factor for heart disease.\n",
    "\n",
    "#### Actionable Insight: Raise awareness about the importance of monitoring blood pressure and the potential need for lifestyle changes or medication among those with elevated readings.\n",
    "\n",
    "#### chol (cholesterol levels): The distribution shows that high cholesterol levels are quite common in the dataset.\n",
    "\n",
    "#### Actionable Insight: Screen for high cholesterol, especially in individuals with other risk factors, and promote dietary changes that can help manage cholesterol levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.392088Z"
    }
   },
   "id": "bf2ec23d-7595-456f-9e43-d84aa94214d8"
  },
  {
   "cell_type": "markdown",
   "id": "46050643-d7b2-46a1-9e29-99c85e4df276",
   "metadata": {},
   "source": [
    "#### fbs (fasting blood sugar): There are significantly more individuals with fasting blood sugar below 120 mg/dl, which is a healthy range.\n",
    "#### Actionable Insight: Continue to educate the public on the importance of diet and exercise in managing blood sugar levels.\n",
    "\n",
    "#### exang (exercise-induced angina): A smaller proportion of individuals experience exercise-induced chest pain.\n",
    "#### Actionable Insight: Encourage those with exercise-induced angina to seek further evaluation, as this can be an indicator of underlying heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.393604900Z"
    }
   },
   "id": "7feb479e-bbfa-44a0-9677-e08358c69122"
  },
  {
   "cell_type": "markdown",
   "id": "f3119cbe-bbe3-4d0c-8b74-b7c8860e4cbf",
   "metadata": {},
   "source": [
    "#### Positive correlations with num (diagnosis of heart disease) include cp_asymptomatic, oldpeak (ST depression induced by exercise), and exang. These variables show a moderate relationship with the presence of heart disease.\n",
    "#### Actionable Insight: For patients who report asymptomatic chest pain or those who exhibit ST depression and exercise-induced angina, a more thorough cardiac evaluation may be warranted.\n",
    "\n",
    "#### Negative correlations with num are seen with thalach (maximum heart rate achieved), and cp_atypical angina. Higher heart rates achieved without chest pain during exercise seem to be associated with a lower incidence of heart disease.\n",
    "#### Actionable Insight: Encourage regular physical activity, as it appears to be protective against heart disease. Individuals able to achieve higher heart rates without pain may be at lower risk and have better cardiovascular health.\n",
    "\n",
    "#### Surprisingly, chol shows a negative correlation with heart disease in this analysis, which is contrary to common medical understanding. It could be due to confounding factors or the particular characteristics of the dataset.\n",
    "#### Actionable Insight: Investigate individual cases where cholesterol levels are high but the risk of heart disease is low to understand other protective factors that may be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 假设 heart_disease_data 是已经加载的 DataFrame，并且目标列已经正确处理\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = heart_disease_data.drop(['heart_disease'], axis=1)\n",
    "y = heart_disease_data['heart_disease']\n",
    "\n",
    "# 识别数值型和分类特征\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 创建预处理转换器\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 创建模型训练管道\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 设置超参数搜索范围\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# 运行网格搜索\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# 使用最佳参数在测试集上进行评估\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.394849200Z"
    }
   },
   "id": "7c48b90f-9388-47a8-b9d9-c20910e0bba1"
  },
  {
   "cell_type": "markdown",
   "id": "546e5e30-c360-4684-94bd-e35f3bf6f29c",
   "metadata": {},
   "source": [
    "#### Analysis for Cholesterol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2f660-c8b0-48aa-a293-426cb818e5da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.396217500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 从特征集中排除'id'列（如果存在）\n",
    "if 'id' in heart_disease_data.columns:\n",
    "    heart_disease_data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# 从特征集中排除原始的目标变量'num'列（如果还未排除）\n",
    "if 'num' in heart_disease_data.columns:\n",
    "    heart_disease_data.drop('num', axis=1, inplace=True)\n",
    "\n",
    "# 分割特征和目标变量\n",
    "X = pd.get_dummies(heart_disease_data.drop('heart_disease', axis=1))  # 对分类变量进行独热编码\n",
    "y = heart_disease_data['heart_disease']\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练XGBoost模型\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 使用SHAP解释模型的预测\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# 可视化SHAP值的水流图\n",
    "shap.initjs()  # 初始化JavaScript环境\n",
    "shap.plots.waterfall(shap_values[0], max_display=14)\n",
    "\n",
    "# 如果想要查看更多样本的SHAP值\n",
    "# for i in range(X_test.shape[0]):\n",
    "#     shap.plots.waterfall(shap_values[i], max_display=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0cea7-63d7-480d-bf88-9e4745d38aa7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.396217500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练XGBoost模型\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性得分并可视化\n",
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y=feature_importances.index, data=feature_importances)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# 计算SHAP值\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "# 可视化SHAP值的汇总图\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "\n",
    "# 获取所有特征的平均SHAP值并可视化\n",
    "shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "importance_df = pd.DataFrame([X_train.columns.tolist(), shap_sum.tolist()]).T\n",
    "importance_df.columns = ['feature', 'shap_importance']\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='shap_importance', y='feature', data=importance_df)\n",
    "plt.title('Average SHAP Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General Actionable Insights:\n",
    "\n",
    "#### Set up community health screenings focusing on blood pressure, cholesterol, and glucose levels, targeting individuals who fall into higher-risk categories based on age and reported symptoms.\n",
    "#### Develop personalized intervention programs that include diet, exercise, and medication where necessary, particularly for those with higher resting blood pressure, cholesterol, and asymptomatic chest pain.\n",
    "#### Initiate educational programs on recognizing signs and symptoms of heart disease, emphasizing the importance of regular health checks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9b020c-262b-4040-87d9-fd160f8783ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aedd2c-fddb-41fb-9498-c7f0a3f5f369",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.398650300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6579df4-7ce2-4932-8c8f-822c2778c224",
   "metadata": {},
   "source": [
    "### Actionable Insights:\n",
    "#### Regular Checkups: Individuals aged 50 and above should undergo regular cholesterol checkups, as cholesterol levels can vary significantly within this age range and are crucial for heart disease prevention.\n",
    "#### Healthy Lifestyle: Promote a healthy lifestyle across all age groups, but especially target educational campaigns for middle-aged individuals, where an increase in cholesterol levels is more pronounced.\n",
    "#### Targeted Interventions: For individuals in the age group where cholesterol levels start rising (based on the boxplot), healthcare providers should consider targeted interventions, such as dietary modifications or physical activity enhancements.\n",
    "#### These insights are derived from the data and should be validated with clinical expertise for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e1505-c648-4572-888b-3b8de6f1b761",
   "metadata": {},
   "source": [
    "### Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74881ce-6126-4719-ad5e-c0b28719b1b1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.399661700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3beeefd4-9c34-4556-825a-052cfaf31790",
   "metadata": {},
   "source": [
    "### Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ede0d-2291-4839-a866-d8d7e79a5c81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.400658800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a4ef85-f8a1-4fca-a395-65963586ef79",
   "metadata": {},
   "source": [
    "## Classification Report Insights:\n",
    "### Class 0 (No Heart Disease) Prediction is Relatively Good:\n",
    "#### The precision, recall, and F1-score for class 0 are reasonably high. This indicates that the model is good at identifying patients without heart disease.\n",
    "\n",
    "### Class 1 (Slight Heart Disease) Prediction is Moderate:\n",
    "#### Moderate scores across precision, recall, and F1 suggest the model has room for improvement in identifying patients with slight heart disease.\n",
    "\n",
    "### Classes 2, 3, and 4 (Moderate to Severe Heart Disease) Prediction is Poor:\n",
    "#### Low precision and recall values for classes 2, 3, and 4 suggest that the model struggles to correctly identify patients with moderate to severe heart disease.\n",
    "\n",
    "### Imbalance in Dataset:\n",
    "#### The 'support' column, which indicates the number of true instances for each class, shows that the dataset may be imbalanced, with fewer instances of more severe heart disease (classes 3 and 4).\n",
    "\n",
    "## Confusion Matrix Insights:\n",
    "### High True Negatives for Class 0:\n",
    "#### A significant number of true negatives for class 0 (no heart disease) indicate that the model can distinguish non-cases well.\n",
    "\n",
    "### Misclassifications Among Classes:\n",
    "#### There are misclassifications, particularly between classes 0 and 1, and between classes 2, 3, and 4, indicating potential confusion in distinguishing different levels of heart disease severity.\n",
    "\n",
    "### Limited Data for Severe Cases:\n",
    "#### There are very few instances for class 4, which might lead to poor model training for this class due to insufficient data.\n",
    "\n",
    "## Actionable Insights:\n",
    "### Model Improvement:\n",
    "#### Investigate feature engineering or more complex models to improve classification of classes 1 to 4, as the current model performance decreases with increasing severity of heart disease.\n",
    "\n",
    "### Data Collection:\n",
    "#### Additional data for underrepresented classes (especially severe heart disease cases) should be collected to improve the model's learning capability for these classes.\n",
    "\n",
    "### Cost-sensitive Learning:\n",
    "#### Given that misclassifications of actual heart disease cases as 'no disease' can be dangerous, consider employing cost-sensitive learning techniques that penalize these types of errors more heavily.\n",
    "\n",
    "### Clinical Corroboration:\n",
    "#### Work closely with clinicians to validate the model's predictions and incorporate clinical expertise into the decision-making process.\n",
    "\n",
    "### Further Diagnostic Testing:\n",
    "#### For patients who are borderline between classes, consider recommending further diagnostic tests to confirm the severity of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8618b7",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f746736",
   "metadata": {},
   "source": [
    "### 1. Identifying the predictor variables and Building the Regression Model by preparing the data, building preprocessing pipeline and train the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9301a8b6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.402040800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159e88f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.403159800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3f092-4a87-47ee-a98d-f7477e59d5ac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.404519200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71691755",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.405994300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4e202",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.405994300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d55343",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.408279200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eded24ca",
   "metadata": {},
   "source": [
    "### The accuracy of a machine learning model is a measure of its ability to correctly classify instances from the dataset. In this case, we trained a machine learning model using various medical and demographic features to predict the presence of heart disease in patients. The accuracy of the model, which is approximately 54.71%, indicates that it correctly predicts the presence or absence of heart disease in about 54.71% of the cases in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602992aa",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "### Finding out which variables are the most impactful in predicting the heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d141b4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.409789800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e45e3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.411245100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c607aa0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.412256400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03339a8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.413625300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6687c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.414636700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bf7f5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.415874700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfabf72",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.417913400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2f625",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.419124300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46916688",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.419124300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1]  # True Positives: Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1]  # False Positives: Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0]  # True Negatives: Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0]  # False Negatives: Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain / (tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain / (tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain / (tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain / (tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot=True, fmt=\".0f\", annot_kws={\"size\": 18})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2a26d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.420391800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1]  # True Positives: Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1]  # False Positives: Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0]  # True Negatives: Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0]  # False Negatives: Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest / (tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest / (tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest / (fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest / (fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred),\n",
    "           annot=True, fmt=\".0f\", annot_kws={\"size\": 18})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ecd40",
   "metadata": {},
   "source": [
    "### Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3730b093",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.421404700Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heart_disease_uci.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Load the dataset (replace \"heart_disease_uci.csv\" with the actual file path)\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m heart_disease_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheart_disease_uci.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Select predictor variables and target variable\u001B[39;00m\n\u001B[0;32m     14\u001B[0m X \u001B[38;5;241m=\u001B[39m heart_disease_df\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum\u001B[39m\u001B[38;5;124m'\u001B[39m])  \u001B[38;5;66;03m# Assuming 'num' is the target column\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'heart_disease_uci.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset (replace \"heart_disease_uci.csv\" with the actual file path)\n",
    "heart_disease_df = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "\n",
    "# Select predictor variables and target variable\n",
    "X = heart_disease_df.drop(columns=['num'])  # Assuming 'num' is the target column\n",
    "y = heart_disease_df['num']\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace NaNs with most frequent value\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))    # Encode categorical features\n",
    "        ]), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators=100, max_depth=4)\n",
    "\n",
    "# Build pipeline with preprocessing and classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rforest)])\n",
    "\n",
    "# Stratified k-fold verification for random forest classifier\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "cv_results = cross_val_score(pipeline, X_train, y_train, cv=skf)\n",
    "print(\"Cross-validation results:\", cv_results)\n",
    "print(\"Mean CV score:\", cv_results.mean())\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy on test data:\", accuracy)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88c138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T10:26:30.102603500Z",
     "start_time": "2024-04-13T10:26:29.872208700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load your heart disease dataset (replace \"heart_disease_df.csv\" with your actual file path)\n",
    "heart_disease_df = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "\n",
    "# Select predictor variables and target variable\n",
    "X = heart_disease_df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                      'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']]\n",
    "y = heart_disease_df['num']\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "numerical_cols = X.columns.drop(categorical_cols)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_cols),  # Impute missing values for numerical columns\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute most frequent value for categorical columns\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_cols)  # One-hot encode categorical columns\n",
    "    ])\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "\n",
    "# Pipeline with preprocessing and classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rforest)])\n",
    "\n",
    "# Stratified k-fold verification for random forest classifier\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "cv_results = cross_val_score(pipeline, X_train, y_train, cv=skf)\n",
    "print(\"Cross-validation results:\", cv_results)\n",
    "print(\"Mean CV score:\", cv_results.mean())\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Now you can evaluate the model and perform further analysis as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a160fc2",
   "metadata": {},
   "source": [
    "### Trying to improve accuracy by hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb016f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T10:26:29.873208700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create pipeline with preprocessor and classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': np.arange(100, 1001, 100),  # Number of trees: 100, 200, ..., 1000\n",
    "    'classifier__max_depth': np.arange(2, 11)               # Depth of trees: 2, 3, 4, ..., 10\n",
    "}\n",
    "\n",
    "# Train the models using cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Fetch the best model or the best set of hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Print the score (accuracy) of the best model after cross-validation\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee005d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
